{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the spreadsheet as a table, Tab, for the exchange-rate data.  It reads in all of the variables.  We get a warning because the first lines have headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mcnel\\AppData\\Local\\Temp/ipykernel_10840/3115810909.py:4: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype={value.dtype})\n",
      "  Tab = pd.read_excel('China_ExRates.xlsx')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>USD-CHF</th>\n",
       "      <th>USD-AUD</th>\n",
       "      <th>RMBI-CHF</th>\n",
       "      <th>RMBI-AUD</th>\n",
       "      <th>RMBI-SGD</th>\n",
       "      <th>RMBI-THB</th>\n",
       "      <th>RMBI-MYR</th>\n",
       "      <th>RMBI-RUB</th>\n",
       "      <th>RMBI-CAD</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 59</th>\n",
       "      <th>Unnamed: 60</th>\n",
       "      <th>Unnamed: 61</th>\n",
       "      <th>Unnamed: 62</th>\n",
       "      <th>Unnamed: 63</th>\n",
       "      <th>Unnamed: 64</th>\n",
       "      <th>Unnamed: 65</th>\n",
       "      <th>Unnamed: 66</th>\n",
       "      <th>Unnamed: 67</th>\n",
       "      <th>Unnamed: 68</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-10-08</td>\n",
       "      <td>0.666533</td>\n",
       "      <td>0.656400</td>\n",
       "      <td>5.51095</td>\n",
       "      <td>5.41135</td>\n",
       "      <td>4.92405</td>\n",
       "      <td>0.20530</td>\n",
       "      <td>2.178421</td>\n",
       "      <td>0.320915</td>\n",
       "      <td>5.62495</td>\n",
       "      <td>...</td>\n",
       "      <td>GBP-Aud</td>\n",
       "      <td>Euro-Aud</td>\n",
       "      <td>KRW-Aud</td>\n",
       "      <td>Dummy</td>\n",
       "      <td>DumDol</td>\n",
       "      <td>THB/AUD</td>\n",
       "      <td>MYR/AUD</td>\n",
       "      <td>SGD/AUD</td>\n",
       "      <td>CAN/AUD</td>\n",
       "      <td>RUB/AUD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-10-11</td>\n",
       "      <td>0.667022</td>\n",
       "      <td>0.653600</td>\n",
       "      <td>5.51520</td>\n",
       "      <td>5.42985</td>\n",
       "      <td>4.93570</td>\n",
       "      <td>0.20785</td>\n",
       "      <td>2.178211</td>\n",
       "      <td>0.320884</td>\n",
       "      <td>5.61985</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003606</td>\n",
       "      <td>-0.005482</td>\n",
       "      <td>-0.002651</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.003561</td>\n",
       "      <td>-0.003817</td>\n",
       "      <td>-0.004637</td>\n",
       "      <td>-0.004888</td>\n",
       "      <td>-0.006488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-10-12</td>\n",
       "      <td>0.673174</td>\n",
       "      <td>0.654600</td>\n",
       "      <td>5.59920</td>\n",
       "      <td>5.43115</td>\n",
       "      <td>4.93430</td>\n",
       "      <td>0.21010</td>\n",
       "      <td>2.178211</td>\n",
       "      <td>0.321008</td>\n",
       "      <td>5.61230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>-0.006734</td>\n",
       "      <td>-0.001894</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.015835</td>\n",
       "      <td>-0.003064</td>\n",
       "      <td>-0.002372</td>\n",
       "      <td>0.003973</td>\n",
       "      <td>-0.004151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-10-13</td>\n",
       "      <td>0.676361</td>\n",
       "      <td>0.654000</td>\n",
       "      <td>5.63075</td>\n",
       "      <td>5.36605</td>\n",
       "      <td>4.91260</td>\n",
       "      <td>0.20930</td>\n",
       "      <td>2.178342</td>\n",
       "      <td>0.322403</td>\n",
       "      <td>5.58495</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000435</td>\n",
       "      <td>-0.004019</td>\n",
       "      <td>0.004697</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.004246</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>-0.003571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-10-14</td>\n",
       "      <td>0.677828</td>\n",
       "      <td>0.650500</td>\n",
       "      <td>5.61180</td>\n",
       "      <td>5.36830</td>\n",
       "      <td>4.92845</td>\n",
       "      <td>0.21010</td>\n",
       "      <td>2.178263</td>\n",
       "      <td>0.321203</td>\n",
       "      <td>5.59075</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.007266</td>\n",
       "      <td>-0.006161</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.013568</td>\n",
       "      <td>-0.00858</td>\n",
       "      <td>-0.008368</td>\n",
       "      <td>-0.004828</td>\n",
       "      <td>0.005352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5192</th>\n",
       "      <td>2019-09-03</td>\n",
       "      <td>1.013346</td>\n",
       "      <td>0.676398</td>\n",
       "      <td>7.26640</td>\n",
       "      <td>4.84930</td>\n",
       "      <td>5.15700</td>\n",
       "      <td>0.23424</td>\n",
       "      <td>1.703700</td>\n",
       "      <td>0.107543</td>\n",
       "      <td>5.37450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005307</td>\n",
       "      <td>0.006839</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007641</td>\n",
       "      <td>0.006793</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>0.005925</td>\n",
       "      <td>0.007957</td>\n",
       "      <td>0.007475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5193</th>\n",
       "      <td>2019-09-04</td>\n",
       "      <td>1.019534</td>\n",
       "      <td>0.679995</td>\n",
       "      <td>7.27570</td>\n",
       "      <td>4.85170</td>\n",
       "      <td>5.15640</td>\n",
       "      <td>0.23354</td>\n",
       "      <td>1.703500</td>\n",
       "      <td>0.107946</td>\n",
       "      <td>5.39640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007449</td>\n",
       "      <td>-0.000487</td>\n",
       "      <td>-0.001332</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>-0.003998</td>\n",
       "      <td>-0.003039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5194</th>\n",
       "      <td>2019-09-05</td>\n",
       "      <td>1.013962</td>\n",
       "      <td>0.681180</td>\n",
       "      <td>7.23860</td>\n",
       "      <td>4.86300</td>\n",
       "      <td>5.15520</td>\n",
       "      <td>0.23265</td>\n",
       "      <td>1.702000</td>\n",
       "      <td>0.108025</td>\n",
       "      <td>5.39610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004297</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>-0.003443</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.006244</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.002761</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.001378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5195</th>\n",
       "      <td>2019-09-08</td>\n",
       "      <td>1.012146</td>\n",
       "      <td>0.684669</td>\n",
       "      <td>7.19340</td>\n",
       "      <td>4.86850</td>\n",
       "      <td>5.14690</td>\n",
       "      <td>0.23157</td>\n",
       "      <td>1.699200</td>\n",
       "      <td>0.108194</td>\n",
       "      <td>5.39850</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.006298</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005108</td>\n",
       "      <td>-0.017388</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>0.000488</td>\n",
       "      <td>-0.001086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>2019-09-09</td>\n",
       "      <td>1.007668</td>\n",
       "      <td>0.686530</td>\n",
       "      <td>7.16790</td>\n",
       "      <td>4.88310</td>\n",
       "      <td>5.15310</td>\n",
       "      <td>0.23203</td>\n",
       "      <td>1.704800</td>\n",
       "      <td>0.108695</td>\n",
       "      <td>5.40140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002353</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.023975</td>\n",
       "      <td>-0.001049</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.00247</td>\n",
       "      <td>-0.000999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5197 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date   USD-CHF   USD-AUD  RMBI-CHF  RMBI-AUD  RMBI-SGD  RMBI-THB  \\\n",
       "0    1999-10-08  0.666533  0.656400   5.51095   5.41135   4.92405   0.20530   \n",
       "1    1999-10-11  0.667022  0.653600   5.51520   5.42985   4.93570   0.20785   \n",
       "2    1999-10-12  0.673174  0.654600   5.59920   5.43115   4.93430   0.21010   \n",
       "3    1999-10-13  0.676361  0.654000   5.63075   5.36605   4.91260   0.20930   \n",
       "4    1999-10-14  0.677828  0.650500   5.61180   5.36830   4.92845   0.21010   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "5192 2019-09-03  1.013346  0.676398   7.26640   4.84930   5.15700   0.23424   \n",
       "5193 2019-09-04  1.019534  0.679995   7.27570   4.85170   5.15640   0.23354   \n",
       "5194 2019-09-05  1.013962  0.681180   7.23860   4.86300   5.15520   0.23265   \n",
       "5195 2019-09-08  1.012146  0.684669   7.19340   4.86850   5.14690   0.23157   \n",
       "5196 2019-09-09  1.007668  0.686530   7.16790   4.88310   5.15310   0.23203   \n",
       "\n",
       "      RMBI-MYR  RMBI-RUB  RMBI-CAD  ...  Unnamed: 59  Unnamed: 60  \\\n",
       "0     2.178421  0.320915   5.62495  ...      GBP-Aud     Euro-Aud   \n",
       "1     2.178211  0.320884   5.61985  ...    -0.003606    -0.005482   \n",
       "2     2.178211  0.321008   5.61230  ...     0.000831    -0.006734   \n",
       "3     2.178342  0.322403   5.58495  ...    -0.000435    -0.004019   \n",
       "4     2.178263  0.321203   5.59075  ...       -0.008    -0.007266   \n",
       "...        ...       ...       ...  ...          ...          ...   \n",
       "5192  1.703700  0.107543   5.37450  ...     0.005307     0.006839   \n",
       "5193  1.703500  0.107946   5.39640  ...    -0.007449    -0.000487   \n",
       "5194  1.702000  0.108025   5.39610  ...    -0.004297     0.002271   \n",
       "5195  1.699200  0.108194   5.39850  ...     0.008361     0.006298   \n",
       "5196  1.704800  0.108695   5.40140  ...    -0.002353     0.000145   \n",
       "\n",
       "      Unnamed: 61  Unnamed: 62  Unnamed: 63  Unnamed: 64  Unnamed: 65  \\\n",
       "0         KRW-Aud        Dummy       DumDol      THB/AUD      MYR/AUD   \n",
       "1       -0.002651            0            0    -0.003561    -0.003817   \n",
       "2       -0.001894            0            0    -0.015835    -0.003064   \n",
       "3        0.004697            0            0     0.004246       0.0066   \n",
       "4       -0.006161            0            0    -0.013568     -0.00858   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "5192     0.005745            1     0.007641     0.006793      0.00465   \n",
       "5193    -0.001332            1     0.005304     0.003572     0.001229   \n",
       "5194    -0.003443            1     0.001742     0.006244     0.001719   \n",
       "5195       0.0013            1     0.005108    -0.017388      0.00245   \n",
       "5196     0.001837            1     0.002715     0.023975    -0.001049   \n",
       "\n",
       "      Unnamed: 66  Unnamed: 67  Unnamed: 68  \n",
       "0         SGD/AUD      CAN/AUD      RUB/AUD  \n",
       "1       -0.004637    -0.004888    -0.006488  \n",
       "2       -0.002372     0.003973    -0.004151  \n",
       "3        0.008368     0.001926    -0.003571  \n",
       "4       -0.008368    -0.004828     0.005352  \n",
       "...           ...          ...          ...  \n",
       "5192     0.005925     0.007957     0.007475  \n",
       "5193     0.000958    -0.003998    -0.003039  \n",
       "5194     0.002761       0.0029     0.001378  \n",
       "5195     0.002827     0.000488    -0.001086  \n",
       "5196     0.001194      0.00247    -0.000999  \n",
       "\n",
       "[5197 rows x 69 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "Tab = pd.read_excel('China_ExRates.xlsx')\n",
    "Tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the size of the Tab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5197, 69)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tab.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the date vector as well as the exchange rate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date        1999-10-08 00:00:00\n",
       "USD-CHF                0.666533\n",
       "USD-AUD                  0.6564\n",
       "RMBI-CHF                5.51095\n",
       "RMBI-AUD                5.41135\n",
       "RMBI-SGD                4.92405\n",
       "RMBI-THB                 0.2053\n",
       "RMBI-MYR                2.17842\n",
       "RMBI-RUB               0.320915\n",
       "RMBI-CAD                5.62495\n",
       "YEN-CHF                   72.11\n",
       "YEN-AUD                    70.7\n",
       "GBP-CHF                0.403015\n",
       "GBP-AUD                0.396936\n",
       "EUR-CHF                0.627156\n",
       "EUR-AUD                0.617627\n",
       "KRW-CHF                  819.89\n",
       "KRW-AUD                  789.46\n",
       "THB-CHF                 26.8877\n",
       "MYR-CHF                  2.5534\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tab.iloc[0,0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import data for the USD, CNY, YEN, GBP, EUR, KRW, THB, SGD,CHF and MYF against CHF, the Swiss Franc, which will be the numeraire currency. Some are flexible rates, some are fixed or quasi-fixed against the dollar.  This will make a big \n",
    "difference.  We look at the Tab to see which column is which for which exchange rate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = Tab.iloc[1:,0];  USDCHF = Tab.iloc[1:,1]; CNYCHF = Tab.iloc[1:,3]; YENCHF = Tab.iloc[1:,10]; \n",
    "GBPCHF = Tab.iloc[1:,12]; EURCHF = Tab.iloc[1:, 14]; KRWCHF = Tab.iloc[1:,16]; THBCHF = Tab.iloc[1:, 18];\n",
    "SGDCHF = Tab.iloc[1:,20]; MYRCHF = Tab.iloc[1:,19];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to transform the variables we imported as Table variables into array variabes, so that we can do numberical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "date2 = np.array(date1); USDCHF1 = np.array(USDCHF); CNYCHF1 = np.array(CNYCHF); YENCHF1 = np.array(YENCHF);\n",
    "GBPCHF1 = np.array(GBPCHF); EURCHF1 = np.array(EURCHF); KRWCHF1 = np.array(KRWCHF); THBCHF1 = np.array(THBCHF);\n",
    "SGDCHF1 = np.array(SGDCHF); MYRCHF1 = np.array(MYRCHF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take log differences of the variables.  This transformation expresses each variable as a daily percentage rate of change. Logs remove the dimensionalty program.  We are interested in how much, percentage-wise, the RMB moves against the Swiss Franc when the dollar moves against the Swiss Franc.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.diff(np.log(CNYCHF1))\n",
    "x = np.diff(np.log([USDCHF1,YENCHF1,GBPCHF1,EURCHF1,KRWCHF1])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created a matrix of regressors x, which inclue the US Dollar, Yen, British pound, Euro and Korean Won against the Swiss franc. We also added in a constant term. Use the same Python codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.488</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.488</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   991.0</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 21 Jul 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:20:07</td>     <th>  Log-Likelihood:    </th>  <td>  19952.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5195</td>      <th>  AIC:               </th> <td>-3.989e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  5189</td>      <th>  BIC:               </th> <td>-3.985e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>-1.819e-05</td> <td> 7.22e-05</td> <td>   -0.252</td> <td> 0.801</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.6612</td> <td>    0.014</td> <td>   47.792</td> <td> 0.000</td> <td>    0.634</td> <td>    0.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0356</td> <td>    0.011</td> <td>    3.291</td> <td> 0.001</td> <td>    0.014</td> <td>    0.057</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0296</td> <td>    0.016</td> <td>    1.886</td> <td> 0.059</td> <td>   -0.001</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0960</td> <td>    0.020</td> <td>    4.779</td> <td> 0.000</td> <td>    0.057</td> <td>    0.135</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0331</td> <td>    0.009</td> <td>    3.808</td> <td> 0.000</td> <td>    0.016</td> <td>    0.050</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1508.823</td> <th>  Durbin-Watson:     </th>  <td>   2.816</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>204719.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.093</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>33.753</td>  <th>  Cond. No.          </th>  <td>    309.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.488\n",
       "Model:                            OLS   Adj. R-squared:                  0.488\n",
       "Method:                 Least Squares   F-statistic:                     991.0\n",
       "Date:                Thu, 21 Jul 2022   Prob (F-statistic):               0.00\n",
       "Time:                        15:20:07   Log-Likelihood:                 19952.\n",
       "No. Observations:                5195   AIC:                        -3.989e+04\n",
       "Df Residuals:                    5189   BIC:                        -3.985e+04\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const      -1.819e-05   7.22e-05     -0.252      0.801      -0.000       0.000\n",
       "x1             0.6612      0.014     47.792      0.000       0.634       0.688\n",
       "x2             0.0356      0.011      3.291      0.001       0.014       0.057\n",
       "x3             0.0296      0.016      1.886      0.059      -0.001       0.060\n",
       "x4             0.0960      0.020      4.779      0.000       0.057       0.135\n",
       "x5             0.0331      0.009      3.808      0.000       0.016       0.050\n",
       "==============================================================================\n",
       "Omnibus:                     1508.823   Durbin-Watson:                   2.816\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           204719.252\n",
       "Skew:                           0.093   Prob(JB):                         0.00\n",
       "Kurtosis:                      33.753   Cond. No.                         309.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "x = sm.add_constant(x)\n",
    "model = sm.OLS(y, x)\n",
    "result1 = model.fit()\n",
    "result1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Chinese RMB (against the Franc) closely following the US dollar against the Franc. If the dollar depreciates against the Franc by one percent, the RMB depreciates by .68 percent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14036047829272022"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ehat = result1.resid\n",
    "SSE = ehat.T.dot(ehat)\n",
    "SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the constant term is insignificant.  The coefficient for the US dollar is significant and large.  The largest weight on a currency after the US dollar is on the Euro, with a weight of .09.  The weights on the Yen, the GBP, and the KRWon are small but significant.  The overall R-squared is .488 and the corrected R-squared is just slightly lower.  The overeall sum of squared residuals is 0.1404.\n",
    "\n",
    "Now we can test the joint significance of the variables other than the dollar for the Chinese exchange-rate management of the RMB against the Swiss Franc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\underline{T-Statistics}$\n",
    "\n",
    "To test a hypothesis about an estimated coefficient, $\\hat{\\beta}_{k}$,  we construct the corresponding t-statistic, based on the deviation of the random variable $\\hat{\\beta}_{k}$  from its (hypothesized) true mean, $\\beta_{k0}$, divided by its standard deviation, $\\hat{\\sigma}_{\\hat{\\beta}_{k}}$:\n",
    "\n",
    "$t_{\\hat{\\beta}_{k}} = \\frac{\\hat{\\beta}_{k} - \\beta_{k}}{\\hat{\\sigma}_{\\hat{\\beta}_{k}}}$\n",
    "\n",
    "\n",
    "\n",
    "The standard deviation $\\hat{\\sigma}_{\\hat{\\beta}_{k}}$ is simply the square root of the variance of $\\hat{\\beta}_{k}$:\n",
    "\n",
    "$\\hat{\\sigma}_{\\hat{\\beta}_{k}} = \\hat{\\sigma} *(x_{k}'x_{k})^{-1}$;\n",
    "\n",
    "The probability distribution of the t-statistic is function of the degrees of freedom of regression equation as well as the computed t-value, and makes use of the Gamma function, $\\Gamma$.  \n",
    "\n",
    "Letting $\\nu = N-k$, the probability distribution of the t-statistic has the following functional form:\n",
    "\n",
    "$\\textbf{Pr}(t_{\\hat{\\beta}}) = \\frac{\\Gamma[.5(\\nu+1)]}{\\sqrt{\\nu\\pi}\\Gamma(.5\\nu)} (1+\\frac{t_{\\hat{beta}}^{2}}{\\nu})^{-.5(\\nu+1)} $\n",
    "\n",
    "\n",
    "The probability distribution quickly converges to that of a normal distribution as the degrees of freedom increase. \n",
    "\n",
    "The Gamma function is an interpolation for positive real numbers of the factorial function for integers.  Thus for a given positive integer v,\n",
    "\n",
    "$\\Gamma(v+1) = v!$\n",
    "\n",
    "\n",
    "\n",
    "$\\underline{Hypothesis\\,Testing\\,on\\,Parameters}$\n",
    "\n",
    "\n",
    "In classical statistical decision making, for a given hypothesis, $H_{0} = \\beta_{k} = \\beta_{k0}$, there are four decisions, two correct ones and two incorrect ones.  The two correct ones are to accept the null if it is true in the real world, and reject it if it is false.  Rejecting the null when it is false is called a Type I error, the probability denoted by $\\boldsymbol\\alpha$, and accepting it when it is false is call a Type II error, it probability denoted by $\\boldsymbol\\beta$.\n",
    "\n",
    "\n",
    "Once we have the estimate of the coefficient $\\hat\\beta_{k}$ and its standard deviation, $\\hat{\\sigma}_{\\hat{\\beta}_{k}}$, it is easy to use the T-distribution to test a hypothesis about the underlying true $\\beta_{k}$.  \n",
    "\n",
    "The hypothesis test proceeds in the following steps for $\\textbf{$\\alpha$}$, the Probability of a Type I error,  usually set at .05:\n",
    "\n",
    "1.  Formulate null hypothesis,  $H_(0):\\beta_{k} = \\beta_{k0}$\n",
    "2.  Estimate the coefficient $\\hat{\\beta}_{k}$ and its standard deviation, $\\hat{\\sigma}_{\\hat{\\beta}_{k}}$ with OLS.\n",
    "3.  Set a confidence interval for the cumulative symmetric t-distribution.  Usually we set a two-sided confidence interval of 95%, or a 5% probability of a Type I error, which means selected values for the cumulative t-distribution of [.025, .975] for the degrees of freedom, $\\nu$.  \n",
    "The values we obtain we call the $\\textbf{critical values}$ of the t-distribution,  $[t^{*}_{\\nu,.025}, t^{*}_{\\nu,.975}]$.\n",
    "4.  Construct a confidence interval for the null hypothesis,  $[\\beta_{k0}-t^{*}_{\\nu,.025}\\hat{\\sigma}\\_{\\hat{\\beta}_{k}}, \\beta_{k0}+t^{*}_{\\nu,.975}\\hat{\\sigma}_{\\hat{\\beta}_{k}}]$.  The numbers [.025, .975] represent the t values for $[.5 \\cdot \\boldsymbol\\alpha,  1-.5 \\cdot \\boldsymbol\\alpha]$, respectively.  \n",
    "5.  Decision:  reject null hypothesis $H_{0}$ if $\\hat{\\beta}_{k}$ is outside the confidence interval, otherwise fail to reject.  Note, we never say we $\\textit{accept}$  the null hypothesis, $H_{0}$, we simply say we fail to reject it. \n",
    "\n",
    "Note that the T-tests are tests on each element of the estimated parameter vector, $\\hat{\\beta_{k}}$.  If there is multicollinearity in any significant degree, thought not perfect multicollinearity, these T-statistics can be misleading, since they do not pick up the effect of other variables other that $x_{k}$ on the standard deviation of $\\hat{\\beta}$.  \n",
    "\n",
    "\n",
    "$\\underline{Regression\\,Diagnostics:Goodness\\,of\\,Fit\\,and\\,Overfitting}$\n",
    "\n",
    "\n",
    "\n",
    "In addition to the point estimates and the t-statistics on each coefficient, we obtain from the regression a common measure of \"goodness of fit\" called the $R^{2}$ statistic.\n",
    "\n",
    "$R^{2} =  \\frac{Var(\\hat{y})}{Var(y)} = 1 -  \\frac{Var(\\hat\\epsilon)}{Var(y)}$\n",
    "\n",
    "As see above, this statistic can be interpreted either as the ratio of variance of the explained or predicted dependent variable to the variance of actual observed dependent variable, or as one less the ratio of the variance of the residuals to the variance of the actual dependent variable.  \n",
    "\n",
    "It is a variance-ratio statistic, the measure of the percentage of the total variance explained by the model to the total variance of the dependent variable.  For this reason, $0\\leq{R^{2}}\\leq{1}$. \n",
    "\n",
    "Note however, that this statistic does not make sense if there is no constant term or intercept. Why is this so?  The reason is that an intercept forces both the actual dependent variable, y, and the explained dependent variable, $\\hat{y}$, to have the same mean, with $\\bar{y} = \\bar{\\hat{y}}$.  If there is no intercept, the means are different, so that $R^{2}$ statistic is comparing variances centered around different means.  So one is likely to get statistics which are outside the bounds [0,1].\n",
    "\n",
    "While this statistic is a useful measure of overall goodness of fit, there are limitations and dangers, especially in the multivariate case.  After all, one can always increase the  $R^{2}$ measure by throwing in more regressors.  Why not?\n",
    "\n",
    "The is the classic problem of over-fitting. The basic message is that models that do exceptionally well with in-sample data (data used to estimate the coefficients), often fail miserably when we use the model for forecasting the dependent variable, with data not used in the estimation.\n",
    "\n",
    "To make a trivial point, if we have N observations and we use N regressors, we will always get a perfect $\\textit{in-sample}$ fit, with $R^{2} = 1$. No big deal.  But if we feed in new regressors, for the given estimated vector, $\\hat{\\beta}$, such models will generally fail abysmally for $\\textit{out-of-sample}$ performance.  We would like good estimated models to do well, both in-sample as well as well out-of-sample.\n",
    "\n",
    "For this reason we take up the issue of regularization criteria. \n",
    "\n",
    "\n",
    "\n",
    "$\\underline{Regularization Criteria}$\n",
    "\n",
    "The basic message is that one can always add in more regressors, and thus more parameters to estimate, to get a better fit.  But in good data science, we look for parsimony, we want to favor simpler models over more complex models, with more and more parameters.  Bigger is not always better.\n",
    "\n",
    "So how do we compare models with different number of regressors?  Obviously, the goodness of fit measure $R^{2}$ does not take into account the number of regressors being used.  \n",
    "\n",
    "One measure is the corrected or adjusted R-squared, often denoted as R-bar-squared:\n",
    "\n",
    "$\\bar{R}^{2} = 1-(1-R^{2})\\cdot\\frac{N-1}{N-k}$\n",
    "\n",
    "where N is the number of observations and k is the number of regressors.\n",
    "\n",
    "The adjusted $R^{2}$ statistic is a good start for comparing two models with different numbers of regressors.  Choose the one, not with the better $R^{2}$ but the better $\\bar{R}^{2}$.\n",
    "\n",
    "However, the adjusted $R^{2}$ is not the only statistic for comparing models with different numbers of coefficients or regressors.  Three statistics which often are part of regression output are the Akaike Information Criterion (AIC), the Schwartz Information Criterion (BIC) and the Hannan-Quinn Information Criterion (HQIF).  There statistics are functions of the in-sample sum of squared errors:\n",
    "$SSE = \\hat{\\epsilon}'\\hat{\\epsilon}$\n",
    "\n",
    "as well as the number of parameters, k and the number of observations of the data used in the regression:\n",
    "\n",
    "\n",
    "\n",
    "$AIC = \\ln(\\frac{SSE}{N}) + \\frac{2k}{N}  \\\\\n",
    "BIC = \\ln(\\frac{SSE}{N}) + k\\frac{\\ln(N)}{N} \\\\\n",
    "HQIC = \\ln(\\frac{SSE}{N}) + k\\frac{\\ln[\\ln(N)]}{N}$\n",
    "\n",
    "\n",
    "The point is to choose the model with the lowest values based on the respective criteria.  The three differ by their penalty factors for the number of parameters, k.  The AIC punishes a high number of factors by the value $\\frac{2}{N}$, which many consider too low a penalty factor for using more parameters.  The BIC uses a factor $\\frac{\\ln(N)}{N}$, considered too harsh a penalty factor.  The HQIC falls between the two penalty factors, with a penalty weight on k equal to $\\frac{\\ln[\\ln(N)]}{N}$.\n",
    "\n",
    "\n",
    "If one has several models with different numbers of regressors or independent variables, go for the one with the lowest value.  Of course, the ranking of the models may different among the AIC, BIC and HQIF regularization criteria.  Which one should be used?  My preference, and that of many in the world of high-frequency financial econometrics, would be the HQIC criteria.\n",
    "\n",
    "\n",
    "$\\underline{F-Statistics}$\n",
    "\n",
    "The F-statistic, developed by Sir Rodney Fisher, is an extremely useful statistic in all matters.\n",
    "Here we discuss two uses:  joint hypothesis tests, and testing for structural change.\n",
    "\n",
    "$\\textit{Joint Hypothesis Tests}$\n",
    "\n",
    "As we noted above, the T-statistics may be distorted if the regressors in the x-matrix are not perfectly statistically independent, since the matrix (x'x) is not diagonal when there is some degree of multicollinearity.\n",
    "\n",
    "In many cases with multivariate regression, we find that individual t-statistics will show that a set of variables, individually, are not significant, but taken together, are jointly significant.  \n",
    "\n",
    "One way of testing the $\\textit{joint}$ significance of a subset of regressors, by imposing r zero restrictions on a subset of parameter, is to do the following set of unrestricted and restricted regressions:\n",
    "\n",
    "1. Run an unrestricted regression, for all k regressors, and obtain the sum of squared errors of the base unrestricted regression, $SSE_{u} = \\hat{\\epsilon_{u}}'\\hat{\\epsilon_{u}}$ from the residuals of the regression.  This regressions has (N-k) degrees of freedom.  \n",
    "2.  One then does a restricted regression, eliminating r regressors or x-variables,  so that the matrix is now of dimension (Nx$k^{*}$) with $k^{*}$<k, and $k^{*}+r =k$, for r restrictions on k parameters  Then obtain the sum of squared residuals from this restricted regressions,\n",
    "$SSE_{r} = \\hat{\\epsilon_{r}}'\\hat{\\epsilon_{r}}$.\n",
    "3. Then form the ratio for the F-test of r restrictions, \n",
    "$ F = \\frac{[SSE_{r}-SSE_{u}]/r}{SSE_{u}/N-k}$\n",
    "4. Compute the critical F-statistic, for the 5 percent level of significance, $F^{r}_{N-k}$ \n",
    "5. Reject the null hypothesis of the joint zero restrictions on the r-dimensional subset of coefficients, if the calculate F-statistic, based on the sum of squared residuals of the restricted and unrestricted regressions, is greater than the critical F-statistic under null hypothesis of the zero restrictions.  \n",
    "\n",
    "This F- statistic is extremely useful.  Variables individually may appear to be insignificant but taken together, with their interaction effects, may be very significant.  \n",
    "\n",
    "We will illustrate the use of the F-statistic on our Chinese exchange rate example.  \n",
    "\n",
    "$\\textit{Structural Change}$\n",
    "\n",
    "Another key use of the F-statistic is to test for structural change.  This test is due to Gregory Chow and is called the Chow Test.  It is also known as a test for equality of coefficients across two samples.\n",
    "\n",
    "In our example, below, we know that the People's Bank of China moved from a US dollar based fixed-exchange rate regime to basket indexing, or setting their exchange rate to a basket of several currencies  Of course, what people and politicians and policy makers say they do is often what they do in practice.  Put another way, there may be official policy change, but little change in underlying economic behavior. \n",
    "\n",
    "If we know the timing of a particular change, we can split up a sample of size N into two smaller samples, $N_{1}$ and $N{2}$, with $N_{1}+N_{2}=N$.   We then do three regressions, one with the full sample, with all N observations, and regressions for the first and second smaller samples, $N_{1}$ and $N_{2}$ and calculate the sum of squared errors, SSE, for the three regressions.  Let $SSE_{f}$, $SSE_{1}$, and $SSE_{2}$ denote the sum of squared residuals for the full sample, and the first and second subsets.  The Chow Test has the following expression:\n",
    "\n",
    "$\\textbf{Chow} = \\frac{[SSE_{f}-(SSE_{1}+SSE_{2})]/k}{[SSE_{1}+SSE_{2}]/(N_{1}+N_{2}-2k)}$\n",
    "\n",
    "The Chow statistic under the null hypothesis of equality of coefficients across sub-periods, or no structural change, has an F-distribution with k,N-2k degrees of freedom, since $N_{1}+N_{2}=N$.\n",
    "\n",
    "$\\textbf{Chow} \\sim F^{k}_{N-2k}$\n",
    "  \n",
    "\n",
    "In time series models, we also need to test for serial independence in the residuals.  In cross-section analysis we test for heteroskedasticity.   We will discuss the various tests in the next two chapters, when we discussion multivariate regression with time series and with cross-section data.\n",
    "\n",
    "The importance of having residuals as identically and independently distributed random variables cannot be overstated.  If there are detectable non-random patterns in the residuals, this means that there is significant misspecification error in the model.  Put another way, it means that we have omitted key variables.  We would like the residuals to represent \"white noise\" processes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.479</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.479</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   4777.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 21 Jul 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:23:11</td>     <th>  Log-Likelihood:    </th>  <td>  19905.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5195</td>      <th>  AIC:               </th> <td>-3.981e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  5193</td>      <th>  BIC:               </th> <td>-3.979e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>-7.773e-06</td> <td> 7.28e-05</td> <td>   -0.107</td> <td> 0.915</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.7332</td> <td>    0.011</td> <td>   69.116</td> <td> 0.000</td> <td>    0.712</td> <td>    0.754</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1509.936</td> <th>  Durbin-Watson:     </th>  <td>   2.788</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>194531.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.159</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>32.977</td>  <th>  Cond. No.          </th>  <td>    146.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.479\n",
       "Model:                            OLS   Adj. R-squared:                  0.479\n",
       "Method:                 Least Squares   F-statistic:                     4777.\n",
       "Date:                Thu, 21 Jul 2022   Prob (F-statistic):               0.00\n",
       "Time:                        15:23:11   Log-Likelihood:                 19905.\n",
       "No. Observations:                5195   AIC:                        -3.981e+04\n",
       "Df Residuals:                    5193   BIC:                        -3.979e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const      -7.773e-06   7.28e-05     -0.107      0.915      -0.000       0.000\n",
       "x1             0.7332      0.011     69.116      0.000       0.712       0.754\n",
       "==============================================================================\n",
       "Omnibus:                     1509.936   Durbin-Watson:                   2.788\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           194531.170\n",
       "Skew:                           0.159   Prob(JB):                         0.00\n",
       "Kurtosis:                      32.977   Cond. No.                         146.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xres = x[:,0:2]\n",
    "model1 = sm.OLS(y,xres)\n",
    "result1_restricted = model1.fit()\n",
    "result1_restricted.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14292118485270894"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resid1 = result1_restricted.resid\n",
    "SSEres = resid1.T.dot(resid1)\n",
    "SSEres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the restricted sum of squared errors  is .1429 while the unrestricted sum of squared errors is .1404.  So is there a significant difference?  Is it ok to remove the additional four exchange rates, that of the Yen, GDP, Euro and Korean Won? We can test this with the F-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.099180911680932"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 4;\n",
    "dgf = 5195-6;\n",
    "Fstat_num =  (.1429-.1404)/k;\n",
    "Fstat_den  =  .1404/dgf;\n",
    "Fstat = Fstat_num/Fstat_den\n",
    "Fstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this F-statistic significant?  Can we reject the null that the four eliminated variables do not make any difference? We can do so with the help of the F-distribution for (4,5190) degrees of freedom\n",
    "Type f.cdf? to find the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1102230246251565e-16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-f.cdf(23.1036,6,5190)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to find the critical F-value for Type I error = .05 for the degrees of freedom, (6, 5190)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3736442966119218"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.ppf(.95,k,dgf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the calculated F statistic is well above the critical F value so we can reject the null hypothesis that the four exchange rates are jointly insignificant.  They matter, for sure.  Maybe not as much as the dollar but they are playing a role in the way the Chinese manage their currency against the Swiss Franc, at least."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see when we add in the other exchange rates, the Malaysian Ringgit, the Singapore dollar and the Thai Baht.  These currencies, like the RMB, also follow the dollar so they are not independent of each other.  We are asking for trouble when we use these as regressors.  Lets do it anyway to see what happens.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.diff(np.log(CNYCHF1)) \n",
    "x = np.diff(np.log([USDCHF1, YENCHF1, GBPCHF1, EURCHF1, KRWCHF1, MYRCHF1, SGDCHF1, THBCHF1])).T\n",
    "xx = sm.add_constant(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.539</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.538</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   757.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 15 Feb 2022</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:32:37</td>     <th>  Log-Likelihood:    </th>  <td>  20221.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5195</td>      <th>  AIC:               </th> <td>-4.042e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  5186</td>      <th>  BIC:               </th> <td>-4.036e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>-3.815e-06</td> <td> 6.86e-05</td> <td>   -0.056</td> <td> 0.956</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.4886</td> <td>    0.015</td> <td>   32.377</td> <td> 0.000</td> <td>    0.459</td> <td>    0.518</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0011</td> <td>    0.011</td> <td>    0.098</td> <td> 0.922</td> <td>   -0.020</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>   -0.0069</td> <td>    0.015</td> <td>   -0.462</td> <td> 0.644</td> <td>   -0.036</td> <td>    0.022</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>   -0.0006</td> <td>    0.020</td> <td>   -0.030</td> <td> 0.976</td> <td>   -0.039</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>    0.0140</td> <td>    0.009</td> <td>    1.558</td> <td> 0.119</td> <td>   -0.004</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>   -0.0088</td> <td>    0.002</td> <td>   -3.755</td> <td> 0.000</td> <td>   -0.013</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.3201</td> <td>    0.014</td> <td>   22.293</td> <td> 0.000</td> <td>    0.292</td> <td>    0.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.1016</td> <td>    0.012</td> <td>    8.657</td> <td> 0.000</td> <td>    0.079</td> <td>    0.125</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1609.554</td> <th>  Durbin-Watson:     </th>  <td>   2.794</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>302842.944</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 0.106</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>40.404</td>  <th>  Cond. No.          </th>  <td>    318.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.539\n",
       "Model:                            OLS   Adj. R-squared:                  0.538\n",
       "Method:                 Least Squares   F-statistic:                     757.5\n",
       "Date:                Tue, 15 Feb 2022   Prob (F-statistic):               0.00\n",
       "Time:                        19:32:37   Log-Likelihood:                 20221.\n",
       "No. Observations:                5195   AIC:                        -4.042e+04\n",
       "Df Residuals:                    5186   BIC:                        -4.036e+04\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const      -3.815e-06   6.86e-05     -0.056      0.956      -0.000       0.000\n",
       "x1             0.4886      0.015     32.377      0.000       0.459       0.518\n",
       "x2             0.0011      0.011      0.098      0.922      -0.020       0.022\n",
       "x3            -0.0069      0.015     -0.462      0.644      -0.036       0.022\n",
       "x4            -0.0006      0.020     -0.030      0.976      -0.039       0.038\n",
       "x5             0.0140      0.009      1.558      0.119      -0.004       0.032\n",
       "x6            -0.0088      0.002     -3.755      0.000      -0.013      -0.004\n",
       "x7             0.3201      0.014     22.293      0.000       0.292       0.348\n",
       "x8             0.1016      0.012      8.657      0.000       0.079       0.125\n",
       "==============================================================================\n",
       "Omnibus:                     1609.554   Durbin-Watson:                   2.794\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           302842.944\n",
       "Skew:                           0.106   Prob(JB):                         0.00\n",
       "Kurtosis:                      40.404   Cond. No.                         318.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = sm.OLS(y,xx);\n",
    "result2 = model2.fit()\n",
    "result2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see something very strange. The Chinese weight on the US dollar has dropped, they are now following the Singapore Dollar with a weight of .32 and the Thai baht with a weight of .10. Can this be true?  Do you really think that the People's Bank of China pays almost the same attention to the Sing dollar as they do to the US dollar?  \n",
    "This is a care when we have to bring $\\textit{priors}$ to our econometric analysis.   We have to make use of prior knowledge that these currencies are not independent of each other and the US dollar,  and thus should not be used as independent regressors seeking to explain the behavior of RMB.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now what about structural change?  Did the coefficients change, using the original set of coefficients, before and after the period of Aug. 1, 2005?  Date 1516 marks Aug. 1, 2005 at the index of 1516 (see cell below)  So we will do three regressions and calculate the sum of squared and do a Chow test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.datetime64('2005-08-01T00:00:00.000000000')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date2[1515]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.diff(np.log(CNYCHF1))\n",
    "x = np.diff(np.log([USDCHF1, YENCHF1, GBPCHF1, EURCHF1, KRWCHF1])).T\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "results_all = sm.OLS(y,x).fit()\n",
    "results_before = sm.OLS(y[:1514],x[:1514,:]).fit()\n",
    "results_after = sm.OLS(y[1515:],x[1515:,:]).fit()\n",
    "error_all = results_all.resid\n",
    "error_before = results_before.resid\n",
    "error_after = results_after.resid\n",
    "SSE_all = error_all.T.dot(error_all)\n",
    "SSE_before = error_before.T.dot(error_before)\n",
    "SSE_after = error_after.T.dot(error_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14036047829272022, 0.0357119299288346, 0.09895188016321668]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[SSE_all, SSE_before, SSE_after]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a big jump in the errors, when we used the combined full sample.  This indicates that there is some type of underlying structural break.  We can use the Chow test, with k = 12 degrees of freedom for the numerator and 5196-12 degrees of freedom for the denominator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.549696032018694"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChowNum =  (SSE_all-(SSE_before+SSE_after))/6;\n",
    "ChowDen =   (SSE_before+SSE_after)/(5196-12);\n",
    "Chow = ChowNum/ChowDen;\n",
    "Chow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Chow test is pretty high and we can guess that it is significant, that we can reject the hypothesis of no structural change.  Lets calculate the critical F value under the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1003375837818896"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.ppf(.95, 6, 5196-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the calculated F statistic is far above the critical value under the null hypothesis.So something significant did happen at the PBC (People's Bank of China) exchange-rate department after August. 2005.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Summary}$\n",
    "\n",
    "We explored how F-statistics can be used in the context of multivariate regression for testing the joint significance of several variables at a time as well as for testing for a structural break.\n",
    "We did so in the context of time-series regression.\n",
    "\n",
    "The test for a structural break can also be put to good use in a cross-section context. For example, for uncovering discrimination in labor markets across industries, one can sample the wage changes of men and women, with regressors for age, experience, education, region of the country, macroeconomic conditions.  One way to uncover evidence of discrimination is to test equality of coefficients between the genders, to see if there is different processes at work for wage changes.  \n",
    "\n",
    "We did not discuss dummy variables for examining structural change.  We can do this by simply putting in a regressor, having the value 0 before Aug. 2005 and 1 afterwards.  But the Chow test is more flexible and general.  \n",
    "\n",
    "As an exercise, I would suggest you do the same exercise but use the AUD (Australian Dollar) as the numeraire currency, rather than the Swiss Franc (CHF).   The PBC did say that neither the Aus dollar nor the Swiss franc were in their basket for currency indexing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
